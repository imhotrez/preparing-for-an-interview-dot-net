#### Scope Refinement
###### Пользователи могут:
- Заполнять собственный профиль для подбора подходящей пары
- Загружать свои фото и видео
- Искать профили по фильтру предпочтений и геолокации
- Ставить лайки понравившимся анкетам
- Получать уведомления о лайках
- Чатиться с мэтчами
###### Администрация моджет:
- Собирать события для последующего хранения и обработки для
	- Система рекомендаций
	- Система промо-пушей

#### Functional Requirements
Для пользователей:
- Страница с профилем пользователя
- Страница поиска с фильтром по гео локации и заданным анкетным параметрам
- Страницы других пользователей на которых можно поставить лайк в ожидании мэтча
- Страница с чатом
- Страница с уведомлениями
Для администрации:
- Доступ к харнилищу логов событий, генерируемых пользователями для анализа данных и тренировки моделей.
#### Non-functional Requirements
- Доступность
- Отзывчивость
- Масштабируемость
- Безопасность
- Разделяемость
#### Capacity Estimation
###### Исходные данные:
- ± 100M MAU (Monthly active users)
- ±10M DAU (Daily active users)
- 1M events/sec, 86 b/day 

Для дальнейших расчетов сделаем определнные допущения, для построения формул расчетов. При получении реальных данных будет достаточно подставить в них настоящие данные. И так, допустим что в среднем:
- У каждого пользователя 20 фото, каждое примерно по 4 мегабайта 
- Одно короткое видео весом до 10 мегабайт.
- Пользователь просматривает 10 профилей в день
- Ставит 1 лайк
- Отправляет 50 сообщений в день каждое 100 символов, общаясь только с 1-м мэтчем (все наши пользователи - люди высокой морали!!!).

В исходных данных ничего нет про то, как часто пользователи создают профили и загружают медиаконтент, поэтому просто предположим что соотношение чтения и записи 1 к 100. Тогда каждый пользователь загружает:
- 20 * 10 / 100 = 2 фото в день
- 1 * 10 / 100 = 0.1 видео в день

###### Нагрузка пользовательского трафика:
- 10М (DAU) пользователей * ((20 фото + 1 видео) * 10 профилей + 50 сообщений) / ~100000 секунд в дне = 1 * 10^7 * 26 * 10^1 / 1 * 10^5 = 26 * 10^3 = 26000 RPS - чтение

- Чтение
	- Фото:   20 * 10 * 10 М / 100000 = 20000 RPS
	- Видео: 1 * 10 * 10М / 100000 = 100 RPS
	- Сообщения: 50 * 10М / 100000 = 500 RPS
- Запись
	- Фото: 2 * 10М / 100000 = 200 RPS
	- Видео: 0.1 * 10М / 100000 = 10 RPS
	- Сообщения: 50 * 10М / 100000 = 500 RPS
- Всего: 
	- ~22000 PRS

###### Нагрузка сетевого трафика:
- Фото: 20000 RPS * 4 Mb + 200 RPS * 4 Mb = 80.8 Gb/s
- Видео: 100 RPS * 10 Mb + 10 RPS * 10 Mb= 1,1 Gb/s
- Сообщения: 500 * 100b * 2 (Сообщение проходит через сервер и значит участвует в трафике дважды U-S-U) = 100Kb/s (На фоне остального трафика можно пренебречь данным компонентом как погрешностью, ведь нам важны лишь порядки)

Всего 82 Gb/s
82 Gb/s * 100000 s  * ~400 d = 3280 PB трафика в год.
###### Нагрузка на хранилище:
- Профили: 100M * 2Kb (Текст в профиле. 2000 символов - примерно 3/4 страницы A4) = 200Gb
- Медиа: 100M пользователей * (20 фото * 4 Mb + 1 видео * 10 Mb) = 90 Tb
- Сообщения: 100М пользователей * 50 сообщений в день* ~400 дней * 5 лет * 100b = 1Tb
- Всего: ~92 TB

И вроде бы все.. НО! Мы же идем по-хардкору! Берем в обработку доп условие, что система генериурет 1M events/sec, 86 b/day!!!

Пусть одна запись весит 1 Kb (сжатие и кодирование данных для уменьшения трафика)
Тогда доп нагрузка на:
- Пользовательский трафик: 1M RPS
- Сетевой трафик: 1M RPS * 1Kb = 1Gb/s
- Хранилище: 1Gb/s * 100000s * ~400 дней * 5 лет = 200 Pb 

###### Нагрузка на вычислительные мощности
Один средний инстанс держит 1К RPS на чтение/запись из бд.
Значит для поддрежания основных функций системы нам потребуется:
- Чтение / запись: 22000 RPS / 1000 RPS = 22 инстанса
- Сетевой трафик: ??????
- Хранище ~20 SSD - 4 физических или 20 облачных инстанса

Для хранения статистики ??????
###### Затраты:
- Обработка данных: 20 инстансов * $100 * 12 * 5 * 3 (надежность) = $360000 на сервера
- Трафик: $0.1 /Gb * 3280 PB = ~$328M
- Хранение: $300/TB * 100 TB = $300000 на HDD хранилище 
+ 20% на быстрое: $2000 / TB * 60 TB = $120000 на SSD хранилище

#### High level design
Сразу можно выделить 4 пользовательских страницы/подсистемы и один API для ML команд
Для пользователей:
- Страница управления собственным профилем (Profile UI)  - загрузка текста анкеты и медиа
- Страница поиска (Search UI) 
- Страница для просмотра профилей из выдачи поиска (Profile Viewer UI)
- Страница с чатом
Для ML команд:
- Доступ к хранилищу данных, где собираются все события системы для их анализа и тренировок моделей для рекомендательной системы и системы, стимулирующей активность
- За каждую фичу отвечает свой сервис или несколько сервисов. 
- Между собой сервисы общаюются через очередь сообщений
- Так же очередь используется для сбора логов событий и отправки их в хранилище

Хранение данных:
- Данные пользователей - чувствительная информация, взаимные отношения (лайки-дизлайки) - реляционная база данных. Следуем мему "Use Postgres".
- Медиаконтент: либо закупаем свои физические сервера, либо используем облачные хранилища 
- Чат: колоночная БД типа Cassandra. 
- Поиск я себе представляю аналогично тому, как я решал задачу с поиском ближайших ресторанов: Гео-поиск основан на структуре Quad Tree.
- Для логгирования событий есть несколько подходов к хранению. Чисто для логирования можно использовать Elasticsearch, получая и бд для логов и полнотекстовый поиск по ним для анализа инцедентов. Если же поток событий используется для обработки больших данных, в чем я не силен, то из курса могу взять схему из дизайна интернет-магазина связку Spark Streaming - Hadoop Cluster - Spark Jobs и Jupiter Notebook в качестве рабочего инструмента для дата саентистов и ML-Инженеров. Так же я слышал, что есть подход, когда в качестве предварительного хранилища сырых данных берётся реализация технологи Data Lake. 
- Продуктом работы ML-команды будут два сервиса: 
	- Сервис рекомендаций, который будет частью подсистемы поиска. Он будет сортировать выдачу по убыванию прогнозируемой вероятности мэтча
	- Сервис стимуляции активности. Он будет планировать активности для каждого пользователя, создавать их расписание и по нему отправлять пользователю уведомления через очередь.
#### Component Design
Теперь можно разделить высокоуровнивый дизайн на подсистемы и подробно описать каждую.
У нас будут следующие подсистемы:
- Загрузка пользовательских данных
- Подсистема поиска пары
- Подсистема просмотра профилей
- Подсистема чата
- Подсистема сбора и анализа данных
- Подсистема нотификаций
- Подсистема безопасности, отказоустойчивости

Начнем с последней:
##### Подсистема нотификаций:
- Сервис получает сообщения из очереди и в зависимости от клиента послылает push-уведомление в соответствубющий сервис.
- Для распределения нагрузки и повышения надежности и отказоустойчивости следует поднять несколько инстансов сервиса и поставить между ними и очередью балансир нагрузки.

##### Подсистема загрузки пользовательских данных
- О надежности хранилища позаботится облачное хранилище (Зеркалирование при помощи RAID-массивов)
- Требуется партицирование через CDN для быстрой доставки контента. Партицировать можно так же по зонам регионов, покрытых CDN так согласуя все с квардро-деревьями, т.к. в данном случае подсистема гео-поиска и подсистема хранения медиаконтента объеденены общей особенностью - важна географическая близость. Так что на один Data-центр CDN будет приходиться некий кластер квадро-деревьев. 
- Более сложная история - хранение профилей пользователей. Текстовые данные небольшого объема не так принципиально где хранить. Гораздо важнее равномерное распределение нагрузки между серверами баз данных. Как мы значем, реляционные бд сложно поддаются горизонтальному масштабированию. Мартин Клепман в "Той самой Книге С Кабанчиком" описывает удобный подход: Если нам известны перспективы роста, мы заранее на небольшом количестве машин подымаем по несколько нод БД на каждой и распределяем запись по хэшу первичного ключа по кругу между общим количеством нод. В случае, когда машина физически перестаетс правляться с количеством запросов, очень легко просто перенести часть разделов на другие машины. Таким образом общее количество шардов базы данных с самого начала остается константным. Меняется лишь количество физических машин, на которых они хостятся. Так мы решаем проблему горизонтального масштабирования. Для надежности, добавим для каждого узла репликацию "с одним ведущим узлом". Это повысит надежность и позволит распределить нагрузку на чтение за счет реплик. По сути мы реализуем архитектурный паттерн разделения на чтение и запись (CQRS)
- С сервисом все по-классике: 
	- множество инстансов для горизонтального масштабирования. 
	- баллансиры нагрузки: основной и дублирующий, на случай отказа первого

##### Подсистема поиска
Здесь я опущу детализацию подсистемы хранения профилей, т.к. она была уже описана в предыдущем пунтке. На схеме я её обозначу общим именем Profiles Data
- Листья дерева хранят ID пользователей из бд Users и самую общую информацию, которая выдается в списке найденых, чтобы не ходить далеко в базу. 
- Аналогично сервису поиска ресторанов квадро-дерево чудесно шардируется по городским агломерациям. "Швы" между такими деревьями следует перекрывать дополнительными квадро-деревьями для надежности системы. 
- Когда пользователь указывает своё местоположение в профиле, индексатор берет эти данные и вносит их в квадродерево для дальнейшего поиска.
- Общая работа выглядит таким образом: Поиск исходя из геоданных в фильтре выбирает нужное квадродерево и получает из него список профилей, удовлетворяющих гео-данным фильтра (точка, радиус), затем идет фильтрация по фильтру, который указал пользователь в искомых анкетных данных. Наконец сервис рекомендаций оценивает выборку, и сортирует её таким образом, чтобы в топе были профили, с которым миэтч прогнозируется с наибольшей вероятностью.
- Горизонтальное масштабирование сервисов, 
- баллансиры
- шардирование деревев.
- Кэш для популярных данных

##### Подсистема просмотра профилей
Наверное самая скучная часть. Пользователь заходит на страницы других пользователей, ознакамливается с анкетными данными, просматривает медиаконтент, ставит лайки, ждет мэтча.
Данные об отношениях собираются в реляционной бд.
- Горизонтальное масштабирование сервисов
- Баллансиры
- Кэш
##### Подсистема чата
Мы не планируем реализовать лучший в мире мессенджер, так что изобретать собственны протоколов и других технологий мы не будем. 
- Возьмем механизм поддерживаемых соединений на web-сокетах.
- Мы сами очень высокоморальны, поэтому сообщения будем шифровать end-2-end на стороне клиентов. К сожалению, у меня уже не хватате времени чтобы описать систему выдачи ключей шифрования.
- Зашифрованные сообщения будем хранить в колоночной бд типа Cassandra
- Websocket Manager создает и управляет соединениями для пользователей. Самые активные хранит в кэше.
- Сообщения пользователей попадают в веб-сокет-обработчики, передавая сообщения адресату и копируя их в харнилище сообщений через очередь сообщений.
- Если на другой стороне соединение не активно, сообщение дублируется через очередь в подсистему уведомлений
- Хранилище колоночной БД прекрасно масштабируется и партицируется. Для надежности добавим реплики.
- Последние сообщения будем хранить в кэше
- Горизонтальное масштабирование сервисов
- Баллансиры
##### Подсистема сбора и анализа данных
Как я уже сказал, я не силен в технологиях сборки, хранения и обработки данных. Мои знания ограничены общей информацией о MapReduce и Hadoop, описанных у Клэпмана, а так же общих подходах из курса. Поэтому, будучи эмпириком, выберу общую схему, описанную в курсе, ведь то, что предлагаю те, у кого есть опыт, наверняка построят более жизнеспособную модель, чем человек без опыта и целостных знаний.
- ВСЕ СЕРВИСЫ ЛОГИРУЮТ СОБЫТИЯ, ОТПРАВЛЯЯ ИХ В ОЧЕРЕДЬ. Этого нет ни на одной нарисованной схеме, поэтому здесь я отмечаю это капсом. Это подразумевается исходя из предпосылок задачи.
- Из очереди сообщения попадают в посдсистему мониторинга, в которой всегда можно посмотреть текущее состояние подсистем, а так же в случае возникновения инцидентов, подсистема мониторинга отправит нотификацию о произошедшем.
- Из очереди сообщений события попадают в Spark Streaming, позволяющий обрабатывать поступающую информацию небольшими батчами и складывать в Hadoop Cluster.
- Hadoop Cluster позволяет предоставляет интерфей распределенной обработки больших данных
- Команда анализа данных запускает различные задачи, локализованные в Spark Jobs
- Результатом обработки данных становятся модели, дающие возможность работать две системы, которые были описаны в задании: системы рекомендаций и системы стимуляции активности, взаимодействующие с подсистемой поиска и подсистемой нотификаций соответственно.

##### Подсистема  защиты
- Ограничители нагрузки на систему
- Фаерволл
- Защита от DDoS
- CDN
Говоря словами живого классика: "Вопросов много, реальных ответов всего три". В нашем случае один - облачный сервис, предлставляющий эти возможности. Один из самых популярных - Cloudflare.  


Подсистема 
#### P.S.
Мне не хватило времени чтобы произвести все расчеты нагрузки. Я не рассчитал нагрузку на подсистему, отвечающую за сбор, анализ данных и обучение моделей. Но последние два месяца моей работы были настолько высоко нагружены, что я удивлен, что хватило времени на то, что сделал.
